{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Website Content\n",
    "\n",
    "This Notebook can be run to scrape a hierarchical html website and place the raw output into Azure blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "First we install the dependencies required in ths notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "%pip install python-dotenv\n",
    "%pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load credentials\n",
    "Next we load the environment variables needed by the following cells from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set execution parameters\n",
    "Set the URL of the website to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the website to scrape\n",
    "url = 'https://quotes.toscrape.com/'\n",
    "\n",
    "# Azure Blob Storage connection string and container name\n",
    "connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "\n",
    "print(f\"Connection string: {connection_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the scrape\n",
    "Now we perform the scrape against the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Create BlobServiceClient and container client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Ensure the container exists\n",
    "try:\n",
    "    container_client.create_container()\n",
    "except Exception as e:\n",
    "    print(f\"Container already exists or could not be created: {e}\")\n",
    "\n",
    "# Function to upload a file to Azure Blob Storage\n",
    "def upload_to_azure(file_name, data):\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "    blob_client.upload_blob(data)\n",
    "    print(f\"Uploaded {file_name} to Azure Blob Storage.\")\n",
    "\n",
    "# Recursive function to scrape hierarchical website and save pages\n",
    "def scrape_and_save(url, base_url, visited=set()):\n",
    "    if url in visited:\n",
    "        return\n",
    "\n",
    "    visited.add(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Save HTML content to Azure Blob Storage\n",
    "    file_name = url.replace(base_url, '').strip('/').replace('/', '_') + '.html'\n",
    "    if not file_name:\n",
    "        file_name = 'index.html'\n",
    "    upload_to_azure(file_name, response.text)\n",
    "\n",
    "    # Find all links and recurse\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        link_url = link['href']\n",
    "        # Handle relative URLs\n",
    "        if not link_url.startswith('http'):\n",
    "            link_url = base_url + link_url.strip('/')\n",
    "        # Recurse on the found link\n",
    "        scrape_and_save(link_url, base_url, visited)\n",
    "\n",
    "# Example usage\n",
    "base_url = 'https://example.com'  # Replace with the root of the website you want to scrape\n",
    "start_url = base_url  # The starting point of the website\n",
    "scrape_and_save(start_url, base_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
