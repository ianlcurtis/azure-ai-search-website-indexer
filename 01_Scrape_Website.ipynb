{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Website Content\n",
    "\n",
    "This Notebook can be run to scrape a hierarchical html website and place the raw output into Azure blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set execution parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import json\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "# Define the URL of the website to scrape\n",
    "url = 'https://example.com'\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to recursively scrape hierarchical data\n",
    "def scrape_data(soup):\n",
    "    data = []\n",
    "    # Example: Find all items in a hierarchical structure\n",
    "    for item in soup.find_all('div', class_='item'):\n",
    "        item_data = {}\n",
    "        item_data['title'] = item.find('h2').text\n",
    "        item_data['description'] = item.find('p').text\n",
    "        # Recursively scrape sub-items if they exist\n",
    "        sub_items = item.find_all('div', class_='sub-item')\n",
    "        if sub_items:\n",
    "            item_data['sub_items'] = scrape_data(BeautifulSoup(str(sub_items), 'html.parser'))\n",
    "        data.append(item_data)\n",
    "    return data\n",
    "\n",
    "# Scrape the data from the website\n",
    "scraped_data = scrape_data(soup)\n",
    "\n",
    "# Convert the scraped data to a string (or JSON)\n",
    "scraped_data_str = json.dumps(scraped_data, indent=4)\n",
    "\n",
    "# Azure Blob Storage connection string and container name\n",
    "connection_string = 'your_connection_string'\n",
    "container_name = 'your_container_name'\n",
    "blob_name = 'scraped_data.json'\n",
    "\n",
    "# Create a BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a ContainerClient\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Upload the scraped data to Azure Blob Storage\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "blob_client.upload_blob(scraped_data_str, overwrite=True)\n",
    "\n",
    "print(\"Data scraped and uploaded to Azure Blob Storage successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
